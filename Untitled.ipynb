{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’² Credit Card Approval ðŸ’²\n",
    "\n",
    "<br>\n",
    "\n",
    "In this Notebook we will predict wether a person will pay or not their credit card, using that information, we will discover if a credit card should be approved to that said person.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### This Notebook is divided into the following sections:\n",
    "\n",
    "<br>\n",
    "\n",
    "- Importing \n",
    "- Cleaning\n",
    "- Exploratory Data Visualization\n",
    "- Training\n",
    "- Hyperparameter Tuning\n",
    "- Formatting answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1 Importing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Basic Tools\n",
    "import numpy              as np\n",
    "import pandas             as pd\n",
    "import seaborn            as sns\n",
    "import matplotlib.pyplot  as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Preprocessing Tools\n",
    "from sklearn              import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Models\n",
    "from sklearn.svm           import SVC\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.linear_model  import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors    import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "\n",
    "test = pd.read_csv('conjunto_de_teste.csv', index_col = 'id_solicitante')\n",
    "train = pd.read_csv('conjunto_de_treinamento.csv.zip', index_col = 'id_solicitante')\n",
    "\n",
    "# Creating Target Column\n",
    "test['inadimplente'] =  np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.1 Checking NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "produto_solicitado                   0.000\n",
       "dia_vencimento                       0.000\n",
       "forma_envio_solicitacao              0.000\n",
       "tipo_endereco                        0.000\n",
       "sexo                                 0.000\n",
       "idade                                0.000\n",
       "estado_civil                         0.000\n",
       "qtde_dependentes                     0.000\n",
       "grau_instrucao                       0.000\n",
       "nacionalidade                        0.000\n",
       "estado_onde_nasceu                   0.000\n",
       "estado_onde_reside                   0.000\n",
       "possui_telefone_residencial          0.000\n",
       "codigo_area_telefone_residencial     0.000\n",
       "tipo_residencia                      2.680\n",
       "meses_na_residencia                  7.250\n",
       "possui_telefone_celular              0.000\n",
       "possui_email                         0.000\n",
       "renda_mensal_regular                 0.000\n",
       "renda_extra                          0.000\n",
       "possui_cartao_visa                   0.000\n",
       "possui_cartao_mastercard             0.000\n",
       "possui_cartao_diners                 0.000\n",
       "possui_cartao_amex                   0.000\n",
       "possui_outros_cartoes                0.000\n",
       "qtde_contas_bancarias                0.000\n",
       "qtde_contas_bancarias_especiais      0.000\n",
       "valor_patrimonio_pessoal             0.000\n",
       "possui_carro                         0.000\n",
       "vinculo_formal_com_empresa           0.000\n",
       "estado_onde_trabalha                 0.000\n",
       "possui_telefone_trabalho             0.000\n",
       "codigo_area_telefone_trabalho        0.000\n",
       "meses_no_trabalho                    0.000\n",
       "profissao                           15.485\n",
       "ocupacao                            14.890\n",
       "profissao_companheiro               57.570\n",
       "grau_instrucao_companheiro          64.300\n",
       "local_onde_reside                    0.000\n",
       "local_onde_trabalha                  0.000\n",
       "inadimplente                         0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there is any NaN value\n",
    "\n",
    "# This shows the percentage of information that is Nan\n",
    "train.isna().sum() * 100 / len(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "produto_solicitado                   0.00\n",
       "dia_vencimento                       0.00\n",
       "forma_envio_solicitacao              0.00\n",
       "tipo_endereco                        0.00\n",
       "sexo                                 0.00\n",
       "idade                                0.00\n",
       "estado_civil                         0.00\n",
       "qtde_dependentes                     0.00\n",
       "grau_instrucao                       0.00\n",
       "nacionalidade                        0.00\n",
       "estado_onde_nasceu                   0.00\n",
       "estado_onde_reside                   0.00\n",
       "possui_telefone_residencial          0.00\n",
       "codigo_area_telefone_residencial     0.00\n",
       "tipo_residencia                      2.50\n",
       "meses_na_residencia                  7.24\n",
       "possui_telefone_celular              0.00\n",
       "possui_email                         0.00\n",
       "renda_mensal_regular                 0.00\n",
       "renda_extra                          0.00\n",
       "possui_cartao_visa                   0.00\n",
       "possui_cartao_mastercard             0.00\n",
       "possui_cartao_diners                 0.00\n",
       "possui_cartao_amex                   0.00\n",
       "possui_outros_cartoes                0.00\n",
       "qtde_contas_bancarias                0.00\n",
       "qtde_contas_bancarias_especiais      0.00\n",
       "valor_patrimonio_pessoal             0.00\n",
       "possui_carro                         0.00\n",
       "vinculo_formal_com_empresa           0.00\n",
       "estado_onde_trabalha                 0.00\n",
       "possui_telefone_trabalho             0.00\n",
       "codigo_area_telefone_trabalho        0.00\n",
       "meses_no_trabalho                    0.00\n",
       "profissao                           15.24\n",
       "ocupacao                            13.80\n",
       "profissao_companheiro               57.74\n",
       "grau_instrucao_companheiro          64.20\n",
       "local_onde_reside                    0.00\n",
       "local_onde_trabalha                  0.00\n",
       "inadimplente                         0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum() * 100 / len(test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the colunms `profissao_companheiro` and `grau_instrucao_companheiro`, have a higher than 50% of NaN values, because of that, they'll be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of Dataframes, so all changes are done on both\n",
    "dfs = [test, train]\n",
    "\n",
    "# List o f Columns to be deleted\n",
    "deleted_columns = ['possui_telefone_celular', \n",
    "                   'grau_instrucao', \n",
    "                   'qtde_contas_bancarias_especiais',\n",
    "                   'profissao_companheiro',\n",
    "                   'grau_instrucao_companheiro']\n",
    "\n",
    "# List of the columns that are numerical\n",
    "numerical =        ['idade',  \n",
    "                    'qtde_dependentes',\n",
    "                    'meses_na_residencia', \n",
    "                    'renda_mensal_regular', \n",
    "                    'renda_extra', \n",
    "                    'qtde_contas_bancarias', \n",
    "                    'valor_patrimonio_pessoal',\n",
    "                    'meses_no_trabalho']\n",
    "\n",
    "# List of the columns that are categorical\n",
    "categorical_1  =   ['possui_email', \n",
    "                    'possui_cartao_visa', \n",
    "                    'possui_cartao_mastercard', \n",
    "                    'possui_cartao_diners', \n",
    "                    'possui_cartao_amex', \n",
    "                    'possui_outros_cartoes', \n",
    "                    'possui_carro',   \n",
    "                    'possui_telefone_trabalho',\n",
    "                    'vinculo_formal_com_empresa']\n",
    "\n",
    "categorical_2 =   [ 'profissao', \n",
    "                    'ocupacao',\n",
    "                    'possui_telefone_residencial',\n",
    "                    'produto_solicitado', \n",
    "                    'dia_vencimento', \n",
    "                    'forma_envio_solicitacao', \n",
    "                    'tipo_endereco', \n",
    "                    'sexo', \n",
    "                    'estado_civil', \n",
    "                    'nacionalidade',\n",
    "                    'tipo_residencia'] \n",
    "                    \n",
    "                    \n",
    "# Deletando por enquanto\n",
    "categories_multi = ['estado_onde_nasceu', \n",
    "                    'estado_onde_reside',\n",
    "                    'estado_onde_trabalha', \n",
    "                    'local_onde_reside', \n",
    "                    'local_onde_trabalha',\n",
    "                    'codigo_area_telefone_residencial',\n",
    "                    'codigo_area_telefone_trabalho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning_Pipeline(dfs, deleted_columns, numerical, categories_multi):\n",
    "\n",
    "    for df in dfs:\n",
    "        \n",
    "        # These columns were advised to be deleted\n",
    "        df.drop(columns = list(deleted_columns + categories_multi) , inplace = True)\n",
    "        \n",
    "        df.sexo.replace(' ', 'N', inplace = True)\n",
    "        \n",
    "        for col, content in df.items():\n",
    "            \n",
    "            if col in list(numerical): \n",
    "\n",
    "                df[col].fillna(inplace = True, value = df[col].median())\n",
    "\n",
    "            elif col not in list(numerical + ['inadimplente']):\n",
    "\n",
    "                df[col].fillna(inplace = True, value = df[col].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Pipeline\n",
    "Cleaning_Pipeline(dfs,deleted_columns, numerical, categories_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enconding Columns\n",
    "\n",
    "train = pd.get_dummies(data = train, columns = list(categorical_1 + categorical_2), drop_first= True)\n",
    "\n",
    "\n",
    "test = pd.get_dummies(data = test, columns = list(categorical_1 + categorical_2),  drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Numerical Data\n",
    "\n",
    "train[numerical] = train[numerical].apply(lambda x:(x-x.min()) / (x.max()-x.min()))\n",
    "test[numerical] = test[numerical].apply(lambda x:(x-x.min()) / (x.max()-x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if train and test columns are the same\n",
    "train_list = list(train.columns)\n",
    "test_list = list(test.columns)\n",
    "\n",
    "not_in_train = [item for item in test_list if item not in train_list]\n",
    "not_in_test  = [item for item in train_list if item not in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns that differ on each dataframe\n",
    "for item in not_in_train:\n",
    "    train[item] = np.zeros(len(train))\n",
    "    \n",
    "for item in not_in_test:\n",
    "    test[item] = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the columns' name so they are in the same order\n",
    "train = train.reindex(sorted(train.columns), axis=1)\n",
    "test = test.reindex(sorted(test.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train = train.drop(labels = 'inadimplente', axis = 1)\n",
    "y_train = train.inadimplente\n",
    "\n",
    "X_test = test.drop(labels = 'inadimplente', axis = 1)\n",
    "y_test = test.inadimplente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(max_iter = 1000),\n",
    "          #'Linear Regression': LinearRegression(n_jobs = -1),\n",
    "          #'Random Forest': RandomForestClassifier(),\n",
    "          'SVC_Linear': SVC(max_iter= -1,\n",
    "                            kernel = 'linear',\n",
    "                            C = 1.0),\n",
    "          'SVC_Poly': SVC(max_iter= -1,\n",
    "                              kernel = 'poly'),\n",
    "          'KNeighbors': KNeighborsClassifier()\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models: a dict of different Scikit-Learn machine learning models.\n",
    "    x_train: training data ( no labels)\n",
    "    x_test: test data (no labels)\n",
    "    y_train: training labels\n",
    "    y_test: test labels\n",
    "    '''\n",
    "    \n",
    "    # Make a dict to keep model scores\n",
    "    model_scores = {}\n",
    "    \n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        \n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        #Evaluates the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "        \n",
    "    return model_scores\n",
    "\n",
    "model_scores = fit_and_score(models = models,\n",
    "                            X_train = X_train,\n",
    "                            X_test = X_test,\n",
    "                            y_train = y_train,\n",
    "                            y_test = y_test)\n",
    "'''\n",
    "{'Logistic Regression': 0.4026,\n",
    " 'Linear Regression': 0.0,\n",
    " 'Random Forest': 0.4416,\n",
    " 'SVC': 0.4212}\n",
    " '''\n",
    "\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results were not good, we will perform a Principal Component Analysis so we can perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score_PCA(models,X_train,y_train,X_test,y_test):\n",
    "\n",
    "    for i in [0.91]:\n",
    "    \n",
    "        pca = PCA(n_components = i)\n",
    "        X_train_PCA = pca.fit_transform(X_train)\n",
    "        X_test_PCA = pca.transform(X_test)\n",
    "\n",
    "        # Make a dict to keep model scores\n",
    "        model_scores = {}\n",
    "\n",
    "        # Loop through models\n",
    "        for name, model in models.items():\n",
    "\n",
    "            # Fit the model to the data\n",
    "            model.fit(X_train_PCA,y_train)\n",
    "\n",
    "            #Evaluates the model and append its score to model_scores\n",
    "            model_scores[name + '_' + str(i)] = model.score(X_test_PCA, y_test)\n",
    "\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression_0.91': 0.5174,\n",
       " 'SVC_Linear_0.91': 0.5392,\n",
       " 'SVC_Poly_0.91': 0.5362,\n",
       " 'SVC_Sigmoid_0.91': 0.5072,\n",
       " 'KNeighbors_0.91': 0.5128}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit_score_PCA(models,X_train,y_train,X_test,y_test)\n",
    "\n",
    "'''\n",
    "{'Logistic Regression_0.95': 0.515,\n",
    " 'SVC_Linear_0.95': 0.5278,\n",
    " 'SVC_Poly_0.95': 0.5246,\n",
    " 'SVC_Sigmoid_0.95': 0.4896,\n",
    " 'KNeighbors_0.95': 0.5198}\n",
    " '''\n",
    "\n",
    "# TODO: N_components between 0.9 ~ 0.99\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression_0.93': 0.5148,\n",
       " 'SVC_Linear_0.93': 0.529,\n",
       " 'SVC_Poly_0.93': 0.538,\n",
       " 'SVC_Sigmoid_0.93': 0.4904,\n",
       " 'KNeighbors_0.93': 0.5188}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit_score_PCA(models,X_train,y_train,X_test,y_test)\n",
    "\n",
    "'''\n",
    "{'Logistic Regression_0.95': 0.515,\n",
    " 'Random Forest_0.95': 0.4996,\n",
    " 'SVC_0.95': 0.5278,\n",
    " 'KNeighbors_0.95': 0.5198}\n",
    "'''\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
